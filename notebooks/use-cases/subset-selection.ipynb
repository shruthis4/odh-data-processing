{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fc83d37",
   "metadata": {},
   "source": [
    "# Data Processing: Subset Selection for Dataset Diversity\n",
    "\n",
    "This notebook demonstrates how to use **subset selection** techniques to identify representative samples from large datasets. The implementation uses facility location maximization with embedding-based similarity to select diverse subsets that cover the full distribution of the data.\n",
    "\n",
    "## Use Cases\n",
    "\n",
    "Subset selection is particularly useful for:\n",
    "- Reducing dataset size while maintaining diversity\n",
    "- Selecting training data that covers the full distribution\n",
    "- Creating validation/test sets that represent the full dataset\n",
    "- Identifying core samples for data annotation or review\n",
    "\n",
    "## Requirements\n",
    "\n",
    "**Note:** This notebook requires GPU acceleration for optimal performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bdb3aa",
   "metadata": {},
   "source": [
    "## ðŸ“¦ Installation\n",
    "\n",
    "Install the required packages for subset selection. Run this once per session. If you restart the kernel or change runtimes, re-run this cell before continuing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd53b6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install subset selection dependencies\n",
    "# See scripts/subset_selection/requirements.txt for the full list\n",
    "%pip install -qq -r ../../scripts/subset_selection/requirements.txt\n",
    "\n",
    "# Note: This installs:\n",
    "# - torch, transformers, numpy (ML & embeddings)\n",
    "# - datasets, h5py (data processing)\n",
    "# - submodlib-py (subset selection optimization)\n",
    "# - jinja2, tqdm (templating & progress bars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0279d250",
   "metadata": {},
   "source": [
    "## ðŸ”§ Configuration\n",
    "\n",
    "### Import Required Modules\n",
    "\n",
    "First, let's import the subset selection functionality from the scripts package.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1541a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the project root to the Python path\n",
    "project_root = Path().absolute().parent.parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Import subset selection modules\n",
    "from scripts.subset_selection import (\n",
    "    subset_datasets,\n",
    "    BasicConfig,\n",
    "    EncoderConfig,\n",
    "    TemplateConfig,\n",
    "    SystemConfig,\n",
    "    get_supported_encoders\n",
    ")\n",
    "\n",
    "print(\"âœ“ Successfully imported subset selection modules\")\n",
    "print(f\"âœ“ Project root: {project_root}\")\n",
    "print(f\"âœ“ Supported encoders: {get_supported_encoders()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce2ab1a",
   "metadata": {},
   "source": [
    "### Configure Parameters\n",
    "\n",
    "Configure all the key parameters for subset selection.\n",
    "\n",
    "**Input Files**: Supported formats include JSONL, JSON, CSV, and Parquet  \n",
    "**Subset Sizes**: Use fractions (0.1 = 10%) or absolute counts (1000 = exactly 1000 samples)  \n",
    "**Output Directory**: Where to save results (embeddings, metadata, subset files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff2ad3f",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# INPUT CONFIGURATION\n",
    "# ============================================================================\n",
    "# Specify dataset file(s) to process (JSONL, JSON, CSV, or Parquet)\n",
    "input_files = [\n",
    "    # TODO: Replace with your actual dataset path\n",
    "    # Example: \"path/to/your/dataset.jsonl\"\n",
    "    \"../assets/subset-selection/combined_cut_50x.jsonl\"  # <-- REPLACE THIS PATH\n",
    "]\n",
    "\n",
    "# ============================================================================\n",
    "# SUBSET CONFIGURATION\n",
    "# ============================================================================\n",
    "# Subset sizes: Use fractions (0.1 = 10%) or absolute counts (1000 = exactly 1000 samples)\n",
    "subset_sizes = [0.1, 0.5]\n",
    "\n",
    "# ============================================================================\n",
    "# OUTPUT CONFIGURATION\n",
    "# ============================================================================\n",
    "# Directory to save results (embeddings, metadata, subset files)\n",
    "output_dir = \"subset-selection/output\"\n",
    "\n",
    "# ============================================================================\n",
    "# BASIC CONFIGURATION\n",
    "# ============================================================================\n",
    "batch_size = 100000       # Number of samples to process at once\n",
    "num_folds = 50           # Number of partitions for parallel processing\n",
    "epsilon = 160.0          # Optimization quality vs speed (160.0 for large datasets, 0.1-1.0 for small)\n",
    "combine_files = False    # Combine multiple input files before processing\n",
    "\n",
    "# ============================================================================\n",
    "# SYSTEM CONFIGURATION\n",
    "# ============================================================================\n",
    "num_gpus = None         # Number of GPUs (None = auto-detect)\n",
    "seed = 42               # Random seed for reproducibility\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744ff980",
   "metadata": {},
   "source": [
    "### Display Configuration\n",
    "\n",
    "Let's verify the configured parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7decbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Create output directory\n",
    "output_path = Path(output_dir)\n",
    "output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Display configuration\n",
    "print(\"=\"*80)\n",
    "print(\"SUBSET SELECTION CONFIGURATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nðŸ“ INPUT:\")\n",
    "print(f\"  Files: {len(input_files)}\")\n",
    "for i, f in enumerate(input_files, 1):\n",
    "    print(f\"    {i}. {f}\")\n",
    "\n",
    "print(f\"\\nðŸ“Š SUBSET SIZES:\")\n",
    "print(f\"  {subset_sizes}\")\n",
    "\n",
    "print(f\"\\nðŸ’¾ OUTPUT:\")\n",
    "print(f\"  Directory: {output_path.absolute()}\")\n",
    "\n",
    "print(f\"\\nâš™ï¸  BASIC CONFIGURATION:\")\n",
    "print(f\"  Batch size: {batch_size:,}\")\n",
    "print(f\"  Number of folds: {num_folds}\")\n",
    "print(f\"  Epsilon: {epsilon}\")\n",
    "print(f\"  Combine files: {combine_files}\")\n",
    "\n",
    "print(f\"\\nðŸ–¥ï¸  SYSTEM:\")\n",
    "print(f\"  GPUs: {'auto-detect' if num_gpus is None else num_gpus}\")\n",
    "print(f\"  Seed: {seed}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc409665",
   "metadata": {},
   "source": [
    "### Advanced Configuration (Optional)\n",
    "\n",
    "The following sections provide additional information about encoder and system configuration. These use defaults from the scripts and typically don't need to be changed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd357cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display encoder information from defaults\n",
    "from scripts.subset_selection import EncoderConfig\n",
    "\n",
    "encoder_config = EncoderConfig()\n",
    "\n",
    "print(\"âœ“ Encoder configuration (using defaults from scripts):\")\n",
    "print(f\"  - Encoder type: {encoder_config.encoder_type}\")\n",
    "print(f\"  - Model: {encoder_config.encoder_model}\")\n",
    "print(f\"  - Template: conversation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70565079",
   "metadata": {},
   "source": [
    "## âœ¨ Run Subset Selection\n",
    "\n",
    "Now we'll run the subset selection process with all the configurations set above.\n",
    "\n",
    "The subset selection process includes:\n",
    "\n",
    "1. **Data Loading**: Reading and preprocessing the input files\n",
    "2. **Embedding Generation**: Computing embeddings for all samples using the Arctic encoder\n",
    "3. **Subset Selection**: Using facility location optimization to select diverse subsets\n",
    "4. **Output Generation**: Saving the selected subsets and metadata\n",
    "\n",
    "**Note:** This may take a while depending on dataset size, number of subsets, and available GPU resources. Progress bars will show the status of each step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a04a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*100)\n",
    "print(\"Starting subset selection...\")\n",
    "print(f\"  Input files: {input_files}\")\n",
    "print(f\"  Subset sizes: {subset_sizes}\")\n",
    "print(f\"  Output directory: {output_dir}\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "try:\n",
    "    # Build kwargs dictionary with configured parameters\n",
    "    kwargs = {\n",
    "        \"output_dir\": output_dir,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"num_folds\": num_folds,\n",
    "        \"epsilon\": epsilon,\n",
    "        \"combine_files\": combine_files,\n",
    "        \"seed\": seed,\n",
    "    }\n",
    "    \n",
    "    # Add optional parameters if specified\n",
    "    if num_gpus is not None:\n",
    "        kwargs[\"num_gpus\"] = num_gpus\n",
    "    \n",
    "    # Run subset selection (encoder settings use defaults from scripts)\n",
    "    subset_datasets(\n",
    "        input_files=input_files,\n",
    "        subset_sizes=subset_sizes,\n",
    "        **kwargs\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(f\"âœ“ Subset selection complete!\")\n",
    "    print(f\"âœ“ Results saved to: {Path(output_dir).absolute()}\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nâœ— Error during subset selection: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54132dd",
   "metadata": {},
   "source": [
    "## ðŸ© Additional Resources\n",
    "\n",
    "For more information about subset selection and data processing:\n",
    "\n",
    "- **GitHub Repository**: [Open Data Hub Data Processing](https://github.com/opendatahub-io/odh-data-processing/)\n",
    "- **Subset Selection README**: Check `scripts/subset_selection/README.md` for detailed documentation\n",
    "- **API Documentation**: See `scripts/subset_selection/subset_selection.py` for advanced configuration options\n",
    "\n",
    "### Technical Details\n",
    "\n",
    "The subset selection algorithm uses:\n",
    "- **Facility Location Maximization**: A submodular optimization technique for diverse subset selection\n",
    "- **LazierThanLazyGreedy Optimizer**: An efficient greedy algorithm implementation\n",
    "- **Embedding-based Similarity**: Uses neural embeddings to measure semantic similarity\n",
    "- **Multi-GPU Parallelization**: Automatically distributes work across available GPUs\n",
    "\n",
    "### Acknowledgement\n",
    "This notebook and the subset selection implementation are based on the work from:\n",
    "\n",
    "**DataCurate4LLMs** by [@krishnatejakk](https://github.com/krishnatejakk)  \n",
    "Repository: https://github.com/krishnatejakk/DataCurate4LLMs\n",
    "\n",
    "The original repository provides data curation techniques for language models, including subset selection using submodular optimization and embedding-based similarity. We've adapted and integrated their approach into the Open Data Hub - Data Processing toolkit.\n",
    "\n",
    "### Any Feedback?\n",
    "\n",
    "We'd love to hear if you have any feedback on this or any other notebook in this series! Please [open an issue](https://github.com/opendatahub-io/odh-data-processing/issues) and help us improve our demos.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
