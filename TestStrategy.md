# **ODH Data Processing - Test Strategy Document**

## **1. Overview & Current State**

### **1.1 Purpose**

This document defines the testing strategy for the ODH Data Processing repository, outlining testing approaches for each repository module, quality metrics, and implementation roadmap. The strategy balances comprehensive coverage with practical implementation bandwidth for the repository.

### **1.2 Repository Modules**

The ODH Data Processing repository contains several distinct module types, each requiring tailored testing approaches:

- **Jupyter Notebooks** (Tutorials & Use Cases)
- **Kubeflow Pipelines** (KFP Components)  
- **Python Scripts** (Subset Selection & Utilities)
- **Custom Workbench Images** (Container Builds)
- **Configuration Files** (CI/CD, Dependencies)
- **Documentation** (README, Guides, Examples)

### **1.3 Current Testing Infrastructure**

**Existing Testing Components:**
- ‚úÖ **Pytest Framework** - Notebook parameter validation
- ‚úÖ **Pre-commit Hooks** - Code formatting (ruff) and notebook cleaning (nbstripout)
- ‚úÖ **CI/CD Workflows** - Automated validation and limited-execution(only for document-coversion-standard )
- ‚úÖ **Makefile Targets** - Easy test execution (`make test-notebook-parameters`)
- ‚úÖ **Papermill Integration** - Notebook execution testing

**Testing Gaps:**
- Limited unit test coverage for Python modules
- No integration testing for KFP pipelines
- No container image validation
- No performance/load testing
- No security scanning integration

---

## **2. Testing Strategy by Repository Module**

### **2.1 Repository Module Testing Matrix**

**Status Legend:**
- ‚úÖ **Done** - Implemented and working
- ‚ùå **Not Started** - No implementation yet  
- üîÑ **In Progress** - Partially implemented

| Repository Module | Testing Approach | Must-Have Test Cases | Nice-to-Have | Tools/Environment | Priority |
|-------------------|-----------------|-----------|--------------|--------|----------|
| **Jupyter Notebooks (Tutorials & Use Cases)** | Execution + Validation | Parameter cell validation ‚úÖ, Successful execution ‚ùå | Output validation, Performance benchmarks, Load testing with large datasets, Multi-environment testing, Vulnerability checks ‚ùå  | pytest, papermill, nbformat, AWS EC2,snyk | HIGH |
| **KFP Pipelines (Standard & VLM)** | Component + Integration | Unit tests for components ‚ùå , Pipeline compilation ‚úÖ, Run pipeline with real cluster ‚ùå | Mock data execution, Performance profiling, Resource usage testing | pytest, kfp SDK, unittest.mock | HIGH |
| **Python Scripts - Subset Selection(TODO-talk to Ali)** | Unit + Integration | Function unit tests ‚ùå, CLI interface testing ‚ùå, File I/O validation ‚ùå | Performance benchmarks, Memory usage testing, Large dataset testing | pytest, pytest-mock, pytest-benchmark | HIGH |
| **Custom Workbench Image(may not need it anymore)** | Container Testing | Build success üîÑ, Base functionality ‚ùå, Security scanning ‚ùå ,entry-point scritp validation  ‚ùå| Multi-arch builds, OpenShift AI integration | Docker, hadolint, trivy, pytest-docker | HIGH |
| **Configuration Files** | Validation + Syntax | YAML/TOML syntax validation ‚úÖ, Schema compliance ‚úÖ, Reference validation ‚ùå | Automated updates, Drift detection, Impact analysis | yamllint, jsonschema, custom validators | LOW |
| **Documentation** | Content + Link Validation | Link checking ‚ùå, Code example execution ‚ùå, Formatting validation ‚ùå | Accessibility testing, Translation validation | markdown-link-check, pytest-doctests | LOW |

---

## **3. Quality Metrics & Targets**

### **3.1 Code Coverage Metrics**

**Primary Metrics:**
```yaml
Code Coverage Targets:
  Python Modules:
    Must-Have: 70% line coverage
    Nice-to-Have: 85% line coverage + branch coverage
    
  Notebook Validation:
    Must-Have: 100% notebook parameter validation
    Nice-to-Have: 90% successful execution rate
    
  KFP Components:
    Must-Have: 60% function coverage
    Nice-to-Have: 80% + integration test coverage
```

**Coverage Implementation:**
```bash
# Generate coverage reports
pytest --cov=kubeflow-pipelines --cov=scripts --cov-report=html
pytest --cov=tests --cov-report=xml  # For CI integration

# Coverage thresholds in pyproject.toml
[tool.coverage.run]
source = ["kubeflow-pipelines", "scripts"]
omit = ["*/tests/*", "*/test_*"]

[tool.coverage.report]
fail_under = 70
show_missing = true
```

### **3.2 Quality Gates**

**Must-Have Quality Gates:**
- ‚úÖ All notebooks execute successfully with default parameters
- ‚úÖ 70%+ code coverage for Python modules  
- ‚úÖ Automated dependency vulnerability scanning
- ‚úÖ All KFP pipelines compile without errors
- ‚úÖ Pre-commit hooks pass (formatting, linting)


**Nice-to-Have Quality Gates:**
- ‚≠ê 85%+ code coverage with branch coverage
- ‚≠ê Performance benchmarks within acceptable ranges
- ‚≠ê Cross-platform compatibility testing
- ‚≠ê Documentation links are valid
- ‚≠ê Integration testing with real ODH cluster
- ‚≠ê Accessibility compliance for documentation


### **3.4 Reliability Metrics**

**Test Reliability Targets:**
```yaml
Reliability Metrics:
  CI/CD Success Rate:
    Must-Have: 95% green builds on main branch
    Nice-to-Have: 98% + flaky test detection
    
```
---

## **5. Tools & Infrastructure**

### **5.1 Testing Framework Stack**

**Core Testing Tools:**
```yaml
Primary Framework: pytest
Extensions:
  - pytest-cov (Code coverage)
  - pytest-benchmark (Performance testing)
  - pytest-mock (Mocking and patching)
  - pytest-xdist (Parallel test execution)
  - pytest-html (HTML test reports)
  
Notebook Testing:
  - papermill (Notebook execution)
  - nbformat (Notebook validation) 
  - jupyter-client (Kernel management)
  
Container Testing:
  - docker (Container operations)
  - hadolint (Dockerfile linting)
  - trivy (Security scanning)
  - pytest-docker (Container testing integration)
```
### **5.2 CI/CD Integration**

**GitHub Actions Workflows:**

**Extended Validation Workflow:**
```yaml
# .github/workflows/test-comprehensive.yml
name: Comprehensive Testing

on:
  pull_request:
    branches: [main]
  push:
    branches: [main]

jobs:
  unit-tests:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Run Python Unit Tests
        run: |
          pytest tests/ --cov=kubeflow-pipelines --cov=scripts \
                 --cov-report=xml --cov-report=html
      
      - name: Upload Coverage Reports  
        uses: codecov/codecov-action@v3
        with:
          file: ./coverage.xml
          
  notebook-tests:
    runs-on: ubuntu-latest  
    steps:
      - uses: actions/checkout@v4
      - name: Execute All Notebooks
        run: |
          pytest tests/test_notebook_execution.py -v
          
  container-tests:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Build and Test Container
        run: |
          docker build -t test-workbench custom-workbench-image/
          trivy image --severity HIGH,CRITICAL test-workbench
```